{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm3hxTwWHZwI"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('heart.csv')"
      ],
      "metadata": {
        "id": "YlLFKsiqHtjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "YzfHvaPhJD3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "1q-_ZEXWJU6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "N_F0ArIoJYQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(data=df, x='age', hue='target', kde=True)\n",
        "plt.title('Age Distribution by Target')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qo9qGm6kJh-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=df, x='sex', hue='target')\n",
        "plt.title('Heart Disease Frequency for Sex')\n",
        "plt.xlabel('Sex (0 = Female, 1 = Male)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g3PDRfHPJudJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=df, x='cp', hue='target')\n",
        "plt.title('Heart Disease Frequency According to Chest Pain Type')\n",
        "plt.xlabel('Chest Pain Type')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jQ-WAVrXJ0EB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "neKSYMnVJ5O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Identify feature types\n",
        "categorical_features = ['cp', 'restecg', 'slope', 'ca', 'thal'] # Features to one-hot encode\n",
        "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
        "# Binary features ('sex', 'fbs', 'exang') don't strictly need one-hot encoding or scaling if kept as 0/1\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "\n",
        "# Create preprocessing pipelines for numerical and categorical features\n",
        "# Numerical pipeline: Scale features\n",
        "num_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical pipeline: One-hot encode features\n",
        "cat_pipeline = Pipeline([\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # handle_unknown='ignore' is safer for unseen categories in test set\n",
        "])\n",
        "\n",
        "# Combine pipelines using ColumnTransformer\n",
        "# Apply num_pipeline to numerical features\n",
        "# Apply cat_pipeline to categorical features needing one-hot encoding\n",
        "# Passthrough binary features ('sex', 'fbs', 'exang') or include them if preferred\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', num_pipeline, numerical_features),\n",
        "        ('cat', cat_pipeline, categorical_features),\n",
        "        ('passthrough', 'passthrough', ['sex', 'fbs', 'exang']) # Keep binary features as they are\n",
        "    ],\n",
        "    remainder='drop' # Drop any columns not specified (shouldn't be any here)\n",
        ")\n",
        "\n",
        "# Apply the preprocessing pipeline to the training data\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "\n",
        "# Apply the *fitted* preprocessing pipeline to the testing data\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Get feature names after transformation (useful for some models like Tree-based ones)\n",
        "feature_names_out = preprocessor.get_feature_names_out()\n",
        "print(f\"\\nProcessed feature names ({X_train_processed.shape[1]} features):\\n{feature_names_out}\")\n",
        "\n",
        "\n",
        "print(f\"\\nProcessed Training set shape: {X_train_processed.shape}\")\n",
        "print(f\"Processed Test set shape: {X_test_processed.shape}\")"
      ],
      "metadata": {
        "id": "LVp2_aukJ9w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"Support Vector Machine\": SVC(probability=True, random_state=42) # probability=True for ROC AUC\n",
        "}\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for name, model in models.items():\n",
        "    print(f\"--- Training {name} ---\")\n",
        "    # Train the model\n",
        "    model.fit(X_train_processed, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_train = model.predict(X_train_processed)\n",
        "    y_pred_test = model.predict(X_test_processed)\n",
        "    y_prob_test = model.predict_proba(X_test_processed)[:, 1] # Probabilities for ROC AUC\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
        "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "    report_test = classification_report(y_test, y_pred_test)\n",
        "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
        "    roc_auc = roc_auc_score(y_test, y_prob_test)\n",
        "\n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        \"model\": model,\n",
        "        \"accuracy_train\": accuracy_train,\n",
        "        \"accuracy_test\": accuracy_test,\n",
        "        \"classification_report\": report_test,\n",
        "        \"confusion_matrix\": cm_test,\n",
        "        \"roc_auc\": roc_auc,\n",
        "        \"y_pred\": y_pred_test,\n",
        "        \"y_prob\": y_prob_test\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n--- Results for {name} ---\")\n",
        "    print(f\"Training Accuracy: {accuracy_train:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy_test:.4f}\")\n",
        "    print(f\"Test ROC AUC: {roc_auc:.4f}\")\n",
        "    print(\"Test Classification Report:\\n\", report_test)\n",
        "    print(\"Test Confusion Matrix:\\n\", cm_test)\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'{name} - Confusion Matrix')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "FcojDXXdKSwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display comparison summary\n",
        "print(\"\\n--- Model Comparison ---\")\n",
        "comparison_data = []\n",
        "for name, result in results.items():\n",
        "    comparison_data.append({\n",
        "        \"Model\": name,\n",
        "        \"Test Accuracy\": result['accuracy_test'],\n",
        "        \"Test ROC AUC\": result['roc_auc']\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(comparison_df.sort_values(by='Test ROC AUC', ascending=False))\n",
        "\n",
        "# Plot ROC Curves for all models\n",
        "plt.figure(figsize=(10, 8))\n",
        "for name, result in results.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, result['y_prob'])\n",
        "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {result['roc_auc']:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess') # Dashed line for random guess\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC Curve Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o53rYBK0KoBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate models with F1 Score and AUC-ROC Curve\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Get the trained Random Forest model from the results dictionary\n",
        "rf_model = results['Random Forest']['model']\n",
        "\n",
        "# Now you can use rf_model\n",
        "# Use X_test_processed instead of X_test for prediction\n",
        "f1 = f1_score(y_test, rf_model.predict(X_test_processed))\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "# Use X_test_processed instead of X_test for ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, rf_model.predict_proba(X_test_processed)[:, 1])\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc_score(y_test, rf_model.predict_proba(X_test_processed)[:, 1]):.2f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OAzjrCr2NW6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance from Random Forest\n",
        "importances = rf_model.feature_importances_\n",
        "# Get feature names after transformation from the preprocessor\n",
        "features = preprocessor.get_feature_names_out()  # This line is changed\n",
        "feat_imp_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
        "feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)\n",
        "sns.barplot(data=feat_imp_df, x='Importance', y='Feature')\n",
        "plt.title('Top Feature Importances')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9MAn0Yt9NdBq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}